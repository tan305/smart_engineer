{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import matplotlib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns=80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"Electricity_Production/train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>V</th>\n",
       "      <th>P</th>\n",
       "      <th>RH</th>\n",
       "      <th>E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>273.39</td>\n",
       "      <td>420.12</td>\n",
       "      <td>91.84</td>\n",
       "      <td>57.41</td>\n",
       "      <td>1778.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>195.26</td>\n",
       "      <td>248.88</td>\n",
       "      <td>92.29</td>\n",
       "      <td>35.21</td>\n",
       "      <td>1824.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>377.52</td>\n",
       "      <td>360.42</td>\n",
       "      <td>92.31</td>\n",
       "      <td>27.69</td>\n",
       "      <td>1761.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>171.60</td>\n",
       "      <td>244.98</td>\n",
       "      <td>91.61</td>\n",
       "      <td>56.99</td>\n",
       "      <td>1889.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>222.56</td>\n",
       "      <td>353.70</td>\n",
       "      <td>92.43</td>\n",
       "      <td>54.25</td>\n",
       "      <td>1806.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>74.23</td>\n",
       "      <td>247.86</td>\n",
       "      <td>91.20</td>\n",
       "      <td>53.69</td>\n",
       "      <td>1943.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>197.73</td>\n",
       "      <td>261.00</td>\n",
       "      <td>92.87</td>\n",
       "      <td>49.24</td>\n",
       "      <td>1856.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>122.33</td>\n",
       "      <td>249.24</td>\n",
       "      <td>92.68</td>\n",
       "      <td>49.31</td>\n",
       "      <td>1926.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>206.57</td>\n",
       "      <td>250.20</td>\n",
       "      <td>92.63</td>\n",
       "      <td>49.61</td>\n",
       "      <td>1853.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>356.20</td>\n",
       "      <td>419.04</td>\n",
       "      <td>91.18</td>\n",
       "      <td>45.89</td>\n",
       "      <td>1722.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>221.00</td>\n",
       "      <td>270.54</td>\n",
       "      <td>92.14</td>\n",
       "      <td>52.99</td>\n",
       "      <td>1835.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>75.53</td>\n",
       "      <td>275.22</td>\n",
       "      <td>91.78</td>\n",
       "      <td>56.63</td>\n",
       "      <td>1918.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>297.83</td>\n",
       "      <td>360.42</td>\n",
       "      <td>92.37</td>\n",
       "      <td>35.83</td>\n",
       "      <td>1784.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>193.44</td>\n",
       "      <td>253.68</td>\n",
       "      <td>91.57</td>\n",
       "      <td>42.78</td>\n",
       "      <td>1866.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>414.31</td>\n",
       "      <td>427.32</td>\n",
       "      <td>91.47</td>\n",
       "      <td>27.40</td>\n",
       "      <td>1748.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>99.45</td>\n",
       "      <td>246.06</td>\n",
       "      <td>93.12</td>\n",
       "      <td>58.30</td>\n",
       "      <td>1903.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>331.50</td>\n",
       "      <td>408.48</td>\n",
       "      <td>91.92</td>\n",
       "      <td>43.72</td>\n",
       "      <td>1744.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>374.92</td>\n",
       "      <td>453.60</td>\n",
       "      <td>92.58</td>\n",
       "      <td>32.38</td>\n",
       "      <td>1770.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>304.98</td>\n",
       "      <td>307.14</td>\n",
       "      <td>91.78</td>\n",
       "      <td>43.93</td>\n",
       "      <td>1779.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>283.92</td>\n",
       "      <td>270.54</td>\n",
       "      <td>92.18</td>\n",
       "      <td>27.67</td>\n",
       "      <td>1803.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>314.08</td>\n",
       "      <td>293.88</td>\n",
       "      <td>92.29</td>\n",
       "      <td>25.53</td>\n",
       "      <td>1837.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>98.15</td>\n",
       "      <td>257.10</td>\n",
       "      <td>91.93</td>\n",
       "      <td>55.95</td>\n",
       "      <td>1939.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>96.85</td>\n",
       "      <td>258.78</td>\n",
       "      <td>92.66</td>\n",
       "      <td>51.79</td>\n",
       "      <td>1942.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>254.93</td>\n",
       "      <td>339.18</td>\n",
       "      <td>92.70</td>\n",
       "      <td>46.28</td>\n",
       "      <td>1816.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>235.95</td>\n",
       "      <td>247.38</td>\n",
       "      <td>90.35</td>\n",
       "      <td>36.73</td>\n",
       "      <td>1825.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>267.93</td>\n",
       "      <td>360.60</td>\n",
       "      <td>91.89</td>\n",
       "      <td>48.34</td>\n",
       "      <td>1801.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>398.71</td>\n",
       "      <td>437.16</td>\n",
       "      <td>91.30</td>\n",
       "      <td>31.63</td>\n",
       "      <td>1731.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>143.39</td>\n",
       "      <td>268.68</td>\n",
       "      <td>92.11</td>\n",
       "      <td>50.31</td>\n",
       "      <td>1880.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>277.55</td>\n",
       "      <td>348.96</td>\n",
       "      <td>92.47</td>\n",
       "      <td>40.60</td>\n",
       "      <td>1811.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>314.60</td>\n",
       "      <td>347.10</td>\n",
       "      <td>92.10</td>\n",
       "      <td>42.17</td>\n",
       "      <td>1787.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>350.61</td>\n",
       "      <td>403.92</td>\n",
       "      <td>92.21</td>\n",
       "      <td>33.04</td>\n",
       "      <td>1758.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>336.44</td>\n",
       "      <td>380.82</td>\n",
       "      <td>92.00</td>\n",
       "      <td>39.52</td>\n",
       "      <td>1775.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>123.63</td>\n",
       "      <td>242.76</td>\n",
       "      <td>92.62</td>\n",
       "      <td>42.07</td>\n",
       "      <td>1885.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>292.37</td>\n",
       "      <td>352.26</td>\n",
       "      <td>91.80</td>\n",
       "      <td>45.86</td>\n",
       "      <td>1773.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>351.52</td>\n",
       "      <td>442.02</td>\n",
       "      <td>91.53</td>\n",
       "      <td>46.47</td>\n",
       "      <td>1724.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>184.86</td>\n",
       "      <td>227.10</td>\n",
       "      <td>91.93</td>\n",
       "      <td>53.09</td>\n",
       "      <td>1884.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>214.50</td>\n",
       "      <td>267.78</td>\n",
       "      <td>92.78</td>\n",
       "      <td>38.47</td>\n",
       "      <td>1835.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>397.41</td>\n",
       "      <td>418.26</td>\n",
       "      <td>91.72</td>\n",
       "      <td>35.99</td>\n",
       "      <td>1757.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>307.19</td>\n",
       "      <td>351.72</td>\n",
       "      <td>92.40</td>\n",
       "      <td>41.98</td>\n",
       "      <td>1782.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>378.82</td>\n",
       "      <td>369.00</td>\n",
       "      <td>91.79</td>\n",
       "      <td>23.97</td>\n",
       "      <td>1751.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>352.04</td>\n",
       "      <td>396.30</td>\n",
       "      <td>92.48</td>\n",
       "      <td>32.81</td>\n",
       "      <td>1757.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>425.62</td>\n",
       "      <td>409.86</td>\n",
       "      <td>91.84</td>\n",
       "      <td>24.77</td>\n",
       "      <td>1766.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>221.39</td>\n",
       "      <td>263.94</td>\n",
       "      <td>92.86</td>\n",
       "      <td>49.39</td>\n",
       "      <td>1841.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>118.69</td>\n",
       "      <td>240.12</td>\n",
       "      <td>93.73</td>\n",
       "      <td>44.86</td>\n",
       "      <td>1893.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>338.52</td>\n",
       "      <td>426.84</td>\n",
       "      <td>91.97</td>\n",
       "      <td>41.37</td>\n",
       "      <td>1735.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>148.59</td>\n",
       "      <td>268.68</td>\n",
       "      <td>92.13</td>\n",
       "      <td>49.47</td>\n",
       "      <td>1884.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>353.34</td>\n",
       "      <td>426.36</td>\n",
       "      <td>91.65</td>\n",
       "      <td>51.79</td>\n",
       "      <td>1722.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>153.01</td>\n",
       "      <td>240.00</td>\n",
       "      <td>92.83</td>\n",
       "      <td>51.05</td>\n",
       "      <td>1901.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>175.37</td>\n",
       "      <td>250.68</td>\n",
       "      <td>91.88</td>\n",
       "      <td>38.00</td>\n",
       "      <td>1871.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>124.15</td>\n",
       "      <td>228.48</td>\n",
       "      <td>92.67</td>\n",
       "      <td>41.03</td>\n",
       "      <td>1916.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>379.21</td>\n",
       "      <td>439.02</td>\n",
       "      <td>91.81</td>\n",
       "      <td>36.58</td>\n",
       "      <td>1728.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>206.57</td>\n",
       "      <td>263.76</td>\n",
       "      <td>92.18</td>\n",
       "      <td>45.14</td>\n",
       "      <td>1869.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>124.28</td>\n",
       "      <td>249.30</td>\n",
       "      <td>91.15</td>\n",
       "      <td>42.16</td>\n",
       "      <td>1924.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>373.88</td>\n",
       "      <td>361.38</td>\n",
       "      <td>91.82</td>\n",
       "      <td>34.94</td>\n",
       "      <td>1749.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>356.59</td>\n",
       "      <td>389.76</td>\n",
       "      <td>90.88</td>\n",
       "      <td>40.76</td>\n",
       "      <td>1724.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>130.39</td>\n",
       "      <td>249.72</td>\n",
       "      <td>92.23</td>\n",
       "      <td>57.37</td>\n",
       "      <td>1850.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>378.30</td>\n",
       "      <td>467.70</td>\n",
       "      <td>91.82</td>\n",
       "      <td>42.89</td>\n",
       "      <td>1725.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>289.90</td>\n",
       "      <td>343.14</td>\n",
       "      <td>91.49</td>\n",
       "      <td>45.17</td>\n",
       "      <td>1780.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>294.71</td>\n",
       "      <td>364.62</td>\n",
       "      <td>92.51</td>\n",
       "      <td>47.66</td>\n",
       "      <td>1777.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>188.50</td>\n",
       "      <td>322.92</td>\n",
       "      <td>92.44</td>\n",
       "      <td>37.73</td>\n",
       "      <td>1857.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           T       V      P     RH        E\n",
       "0     273.39  420.12  91.84  57.41  1778.12\n",
       "1     195.26  248.88  92.29  35.21  1824.16\n",
       "2     377.52  360.42  92.31  27.69  1761.72\n",
       "3     171.60  244.98  91.61  56.99  1889.64\n",
       "4     222.56  353.70  92.43  54.25  1806.68\n",
       "5      74.23  247.86  91.20  53.69  1943.32\n",
       "6     197.73  261.00  92.87  49.24  1856.32\n",
       "7     122.33  249.24  92.68  49.31  1926.36\n",
       "8     206.57  250.20  92.63  49.61  1853.32\n",
       "9     356.20  419.04  91.18  45.89  1722.16\n",
       "10    221.00  270.54  92.14  52.99  1835.76\n",
       "11     75.53  275.22  91.78  56.63  1918.64\n",
       "12    297.83  360.42  92.37  35.83  1784.76\n",
       "13    193.44  253.68  91.57  42.78  1866.92\n",
       "14    414.31  427.32  91.47  27.40  1748.00\n",
       "15     99.45  246.06  93.12  58.30  1903.56\n",
       "16    331.50  408.48  91.92  43.72  1744.28\n",
       "17    374.92  453.60  92.58  32.38  1770.76\n",
       "18    304.98  307.14  91.78  43.93  1779.28\n",
       "19    283.92  270.54  92.18  27.67  1803.52\n",
       "20    314.08  293.88  92.29  25.53  1837.08\n",
       "21     98.15  257.10  91.93  55.95  1939.72\n",
       "22     96.85  258.78  92.66  51.79  1942.40\n",
       "23    254.93  339.18  92.70  46.28  1816.88\n",
       "24    235.95  247.38  90.35  36.73  1825.28\n",
       "25    267.93  360.60  91.89  48.34  1801.84\n",
       "26    398.71  437.16  91.30  31.63  1731.04\n",
       "27    143.39  268.68  92.11  50.31  1880.68\n",
       "28    277.55  348.96  92.47  40.60  1811.32\n",
       "29    314.60  347.10  92.10  42.17  1787.48\n",
       "...      ...     ...    ...    ...      ...\n",
       "1970  350.61  403.92  92.21  33.04  1758.44\n",
       "1971  336.44  380.82  92.00  39.52  1775.76\n",
       "1972  123.63  242.76  92.62  42.07  1885.28\n",
       "1973  292.37  352.26  91.80  45.86  1773.32\n",
       "1974  351.52  442.02  91.53  46.47  1724.40\n",
       "1975  184.86  227.10  91.93  53.09  1884.20\n",
       "1976  214.50  267.78  92.78  38.47  1835.12\n",
       "1977  397.41  418.26  91.72  35.99  1757.52\n",
       "1978  307.19  351.72  92.40  41.98  1782.04\n",
       "1979  378.82  369.00  91.79  23.97  1751.04\n",
       "1980  352.04  396.30  92.48  32.81  1757.88\n",
       "1981  425.62  409.86  91.84  24.77  1766.64\n",
       "1982  221.39  263.94  92.86  49.39  1841.00\n",
       "1983  118.69  240.12  93.73  44.86  1893.88\n",
       "1984  338.52  426.84  91.97  41.37  1735.12\n",
       "1985  148.59  268.68  92.13  49.47  1884.36\n",
       "1986  353.34  426.36  91.65  51.79  1722.64\n",
       "1987  153.01  240.00  92.83  51.05  1901.96\n",
       "1988  175.37  250.68  91.88  38.00  1871.68\n",
       "1989  124.15  228.48  92.67  41.03  1916.92\n",
       "1990  379.21  439.02  91.81  36.58  1728.28\n",
       "1991  206.57  263.76  92.18  45.14  1869.40\n",
       "1992  124.28  249.30  91.15  42.16  1924.96\n",
       "1993  373.88  361.38  91.82  34.94  1749.68\n",
       "1994  356.59  389.76  90.88  40.76  1724.84\n",
       "1995  130.39  249.72  92.23  57.37  1850.92\n",
       "1996  378.30  467.70  91.82  42.89  1725.12\n",
       "1997  289.90  343.14  91.49  45.17  1780.40\n",
       "1998  294.71  364.62  92.51  47.66  1777.72\n",
       "1999  188.50  322.92  92.44  37.73  1857.08\n",
       "\n",
       "[2000 rows x 5 columns]"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler(with_std=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration no.\n",
      "1\n",
      "[5.9175118965647497, 3.96026393892364, 1.4434566824596824, 1.6658902726010563]\n",
      "Max VIF is for variable no.:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def calculate_vif(x):\n",
    "    thresh = 10.0\n",
    "    output = pd.DataFrame()\n",
    "    k = x.shape[1]\n",
    "    vif = [variance_inflation_factor(x.values, j) for j in range(x.shape[1])]\n",
    "    for i in range(1,k):\n",
    "        print(\"Iteration no.\")\n",
    "        print(i)\n",
    "        print(vif)\n",
    "        a = np.argmax(vif)\n",
    "        print(\"Max VIF is for variable no.:\")\n",
    "        print(a)\n",
    "        if vif[a] <= thresh :\n",
    "            break\n",
    "        if i == 1 :          \n",
    "            output = x.drop(x.columns[a], axis = 1)\n",
    "            vif = [variance_inflation_factor(output.values, j) for j in range(output.shape[1])]\n",
    "        elif i > 1 :\n",
    "            output = output.drop(output.columns[a],axis = 1)\n",
    "            vif = [variance_inflation_factor(output.values, j) for j in range(output.shape[1])]\n",
    "    return(output)\n",
    "train_out = calculate_vif(pd.DataFrame(data[:,:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  17.37918 ,   94.90242 ,   -0.271935,   13.24036 ,  -38.93286 ],\n",
       "       [ -60.75082 ,  -76.33758 ,    0.178065,   -8.95964 ,    7.10714 ],\n",
       "       [ 121.50918 ,   35.20242 ,    0.198065,  -16.47964 ,  -55.33286 ],\n",
       "       ..., \n",
       "       [  33.88918 ,   17.92242 ,   -0.621935,    1.00036 ,  -36.65286 ],\n",
       "       [  38.69918 ,   39.40242 ,    0.398065,    3.49036 ,  -39.33286 ],\n",
       "       [ -67.51082 ,   -2.29758 ,    0.328065,   -6.43964 ,   40.02714 ]])"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2 = sm.OLS(data[:,-1],data[:,:-1]).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.929</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.928</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   6482.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 09 Apr 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>00:41:31</td>     <th>  Log-Likelihood:    </th> <td> -8596.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2000</td>      <th>  AIC:               </th> <td>1.720e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1996</td>      <th>  BIC:               </th> <td>1.722e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th> <td>   -0.5964</td> <td>    0.010</td> <td>  -58.561</td> <td> 0.000</td> <td>   -0.616</td> <td>   -0.576</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th> <td>   -0.1625</td> <td>    0.010</td> <td>  -15.627</td> <td> 0.000</td> <td>   -0.183</td> <td>   -0.142</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th> <td>    2.1803</td> <td>    0.888</td> <td>    2.455</td> <td> 0.014</td> <td>    0.439</td> <td>    3.922</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th> <td>   -1.0254</td> <td>    0.059</td> <td>  -17.490</td> <td> 0.000</td> <td>   -1.140</td> <td>   -0.910</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>193.445</td> <th>  Durbin-Watson:     </th> <td>   1.998</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 888.329</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.350</td>  <th>  Prob(JB):          </th> <td>1.26e-193</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.189</td>  <th>  Cond. No.          </th> <td>    262.</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.929\n",
       "Model:                            OLS   Adj. R-squared:                  0.928\n",
       "Method:                 Least Squares   F-statistic:                     6482.\n",
       "Date:                Mon, 09 Apr 2018   Prob (F-statistic):               0.00\n",
       "Time:                        00:41:31   Log-Likelihood:                -8596.3\n",
       "No. Observations:                2000   AIC:                         1.720e+04\n",
       "Df Residuals:                    1996   BIC:                         1.722e+04\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1            -0.5964      0.010    -58.561      0.000      -0.616      -0.576\n",
       "x2            -0.1625      0.010    -15.627      0.000      -0.183      -0.142\n",
       "x3             2.1803      0.888      2.455      0.014       0.439       3.922\n",
       "x4            -1.0254      0.059    -17.490      0.000      -1.140      -0.910\n",
       "==============================================================================\n",
       "Omnibus:                      193.445   Durbin-Watson:                   1.998\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              888.329\n",
       "Skew:                          -0.350   Prob(JB):                    1.26e-193\n",
       "Kurtosis:                       6.189   Cond. No.                         262.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence = lm2.get_influence()  \n",
    "resid_student = influence.resid_studentized_external"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>Studentized Residuals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.37918</td>\n",
       "      <td>94.90242</td>\n",
       "      <td>-0.271935</td>\n",
       "      <td>13.24036</td>\n",
       "      <td>0.057265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-60.75082</td>\n",
       "      <td>-76.33758</td>\n",
       "      <td>0.178065</td>\n",
       "      <td>-8.95964</td>\n",
       "      <td>-2.875568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>121.50918</td>\n",
       "      <td>35.20242</td>\n",
       "      <td>0.198065</td>\n",
       "      <td>-16.47964</td>\n",
       "      <td>0.310358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-84.41082</td>\n",
       "      <td>-80.23758</td>\n",
       "      <td>-0.501935</td>\n",
       "      <td>12.82036</td>\n",
       "      <td>1.317905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-33.45082</td>\n",
       "      <td>28.48242</td>\n",
       "      <td>0.318065</td>\n",
       "      <td>10.08036</td>\n",
       "      <td>-0.901526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3  Studentized Residuals\n",
       "0   17.37918  94.90242 -0.271935  13.24036               0.057265\n",
       "1  -60.75082 -76.33758  0.178065  -8.95964              -2.875568\n",
       "2  121.50918  35.20242  0.198065 -16.47964               0.310358\n",
       "3  -84.41082 -80.23758 -0.501935  12.82036               1.317905\n",
       "4  -33.45082  28.48242  0.318065  10.08036              -0.901526"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resid=pd.DataFrame(data[:,:-1])\n",
    "resid[\"Studentized Residuals\"]=resid_student\n",
    "resid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([71, 83, 228, 274, 808, 1749], dtype='int64')"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = resid.loc[np.absolute(resid[\"Studentized Residuals\"]) > 3.5,:].index\n",
    "ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"Electricity_Production/train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame(data).drop(ind,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data[\"RH\"]=boxcox(data[\"RH\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_X=StandardScaler(with_std=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_y=StandardScaler(with_std=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train=scaler_X.fit_transform(data[data.columns[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=scaler_y.fit_transform(data[data.columns[-1]].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin=LinearRegression(normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93401241005893054"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(estimator=lin,X=X_train,y=y_train,scoring='r2',cv=3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso=Lasso(random_state=0,alpha=.1,normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=0,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93400795061520492"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(estimator=lasso,X=X_train,y=y_train,scoring='r2',cv=3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge=Ridge(alpha=.001,random_state=0,normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9340062455527699"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(estimator=ridge,X=X_train,y=y_train,scoring='r2',cv=3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=True, random_state=0, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm=RandomForestRegressor(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94666982961083723"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(estimator=rm,X=X_train,y=y_train,scoring='r2',cv=3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm=GradientBoostingRegressor(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9499684339250446"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(estimator=gbm,X=X_train,y=y_train,scoring='r2',cv=3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=0,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 95}\n",
      "0.9532693891238202\n"
     ]
    }
   ],
   "source": [
    "params={\"n_estimators\":(60,70,80,90,95,100,105,110,115,120,130)}\n",
    "\n",
    "gsearch=GridSearchCV(cv=3,estimator=GradientBoostingRegressor(random_state=0,learning_rate=0.1, min_samples_split=10,min_samples_leaf=2,max_depth=8,max_features='sqrt',subsample=0.8),scoring=\"r2\",param_grid=params,n_jobs=-1)\n",
    "\n",
    "gsearch.fit(X_train,y_train)\n",
    "\n",
    "print(gsearch.best_params_)\n",
    "print(gsearch.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 7, 'min_samples_split': 20, 'min_samples_leaf': 3}\n",
      "0.9540959156260437\n"
     ]
    }
   ],
   "source": [
    "params={\"max_depth\":(4,5,6,7,8,9,10),\"min_samples_split\":(12,14,16,18,20,22,24,26,28),\"min_samples_leaf\":(2,3,4,5,6,8)}\n",
    "\n",
    "gsearch1=GridSearchCV(cv=3,estimator=GradientBoostingRegressor(n_estimators=95,random_state=0,learning_rate=0.1,max_features='sqrt',subsample=0.8),scoring=\"r2\",param_grid=params,n_jobs=-1)\n",
    "\n",
    "gsearch1.fit(X_train,y_train)\n",
    "\n",
    "print(gsearch1.best_params_)\n",
    "print(gsearch1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 2}\n",
      "0.9540959156260437\n"
     ]
    }
   ],
   "source": [
    "params={\"max_features\":(2,3,4)}\n",
    "\n",
    "gsearch2=GridSearchCV(cv=3,estimator=GradientBoostingRegressor(n_estimators=95,random_state=0,learning_rate=0.1, min_samples_split=20,min_samples_leaf=3,max_depth=7,subsample=0.8),scoring=\"r2\",param_grid=params,n_jobs=-1)\n",
    "\n",
    "gsearch2.fit(X_train,y_train)\n",
    "\n",
    "print(gsearch2.best_params_)\n",
    "print(gsearch2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subsample': 0.8}\n",
      "0.9540959156260437\n"
     ]
    }
   ],
   "source": [
    "params={\"subsample\":(.5,.6,.75,0.8,0.85,.9)}\n",
    "\n",
    "gsearch3=GridSearchCV(cv=3,estimator=GradientBoostingRegressor(n_estimators=95,random_state=0,learning_rate=0.1, min_samples_split=20,min_samples_leaf=3,max_depth=7,max_features=2),scoring=\"r2\",param_grid=params,n_jobs=-1)\n",
    "\n",
    "gsearch3.fit(X_train,y_train)\n",
    "\n",
    "print(gsearch3.best_params_)\n",
    "print(gsearch3.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 2880, 'learning_rate': 0.005}\n",
      "0.9555617473612292\n"
     ]
    }
   ],
   "source": [
    "params={\"learning_rate\":(.1,.05,.01,.005,.001,.0005,.0001),\"n_estimators\":(90,180,360,720,1440,2880,2880*2)}\n",
    "\n",
    "gsearch3=GridSearchCV(cv=3,estimator=GradientBoostingRegressor(random_state=0, min_samples_split=20,min_samples_leaf=3,max_depth=7,max_features=2,subsample=0.8),scoring=\"r2\",param_grid=params,n_jobs=-1)\n",
    "\n",
    "gsearch3.fit(X_train,y_train)\n",
    "\n",
    "print(gsearch3.best_params_)\n",
    "print(gsearch3.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_gbm=GradientBoostingRegressor(random_state=0, min_samples_split=20,min_samples_leaf=3,max_depth=7,max_features=2,subsample=0.8,n_estimators=2880,learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95548015684615561"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(estimator=final_gbm,X=X_train,y=y_train,scoring='r2',cv=3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.005, loss='ls', max_depth=7, max_features=2,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=3,\n",
       "             min_samples_split=20, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=2880, presort='auto', random_state=0,\n",
       "             subsample=0.8, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_gbm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb=XGBRegressor(seed=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.95057603,  0.94795459,  0.95146734])"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(estimator=xgb,X=X_train,y=y_train,scoring='r2',cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 400}\n",
      "0.9539553648312867\n"
     ]
    }
   ],
   "source": [
    "params={\"n_estimators\":(300,325,350,375,395,400,405,425,450,475,500,525,550)}\n",
    "\n",
    "xgsearch=GridSearchCV(cv=3,estimator=XGBRegressor(learning_rate =0.1, max_depth=5,min_child_weight=1,gamma=0, subsample=0.8, colsample_bytree=0.8, nthread=4, scale_pos_weight=1, seed=27),scoring=\"r2\",param_grid=params,n_jobs=-1)\n",
    "\n",
    "xgsearch.fit(X_train,y_train)\n",
    "\n",
    "print(xgsearch.best_params_)\n",
    "print(xgsearch.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 4, 'min_child_weight': 3}\n",
      "0.9547815999287047\n"
     ]
    }
   ],
   "source": [
    "param_test2 = {\n",
    " 'max_depth':(1,2,3,4,5,6,7,8,10),\n",
    " 'min_child_weight':(1,2,3,4,6,8,10)\n",
    "}\n",
    "xgsearch2 = GridSearchCV(estimator = XGBRegressor( n_estimators=400,learning_rate =0.1,gamma=0, subsample=0.8, colsample_bytree=0.8, nthread=4, scale_pos_weight=1, seed=27), \n",
    "param_grid = param_test2, scoring='r2',n_jobs=-1,iid=False, cv=3)\n",
    "xgsearch2.fit(X_train,y_train)\n",
    "print(xgsearch2.best_params_)\n",
    "print(xgsearch2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.2}\n",
      "0.9547896148894243\n"
     ]
    }
   ],
   "source": [
    "param_test3 = {'gamma':[i/10.0 for i in range(0,5)]}\n",
    "xgsearch3 = GridSearchCV(estimator = XGBRegressor(learning_rate=0.1, n_estimators=400, max_depth=4,\n",
    "min_child_weight=3, subsample=0.8, colsample_bytree=0.8, nthread=4, scale_pos_weight=1,seed=27), \n",
    "param_grid = param_test3, scoring='r2',n_jobs=4,iid=False, cv=3)\n",
    "xgsearch3.fit(X_train,y_train)\n",
    "print(xgsearch3.best_params_)\n",
    "print(xgsearch3.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.75, 'subsample': 0.8}\n",
      "0.9547896148894243\n"
     ]
    }
   ],
   "source": [
    "param_test4 = {\n",
    " 'subsample':(.75,.8,.85,.90),\n",
    " 'colsample_bytree':(.5,.55,.6,.65,.7,.75,.8,.9)\n",
    "}\n",
    "xgsearch4 = GridSearchCV(estimator = XGBRegressor( learning_rate =0.1, n_estimators=400, max_depth=4,gamma=0.2,\n",
    "min_child_weight=3, nthread=4, scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test4, scoring='r2',n_jobs=4,iid=False, cv=3)\n",
    "xgsearch4.fit(X_train,y_train)\n",
    "print(xgsearch4.best_params_)\n",
    "print(xgsearch4.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reg_alpha': 1e-05}\n",
      "0.9547896152670255\n"
     ]
    }
   ],
   "source": [
    "param_test5 = {'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]}\n",
    "xgsearch5 = GridSearchCV(estimator = XGBRegressor( learning_rate =0.1, n_estimators=400, max_depth=4,gamma=0.2,colsample_bytree=.75,subsample=.8,\n",
    "min_child_weight=3, nthread=4, scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test5, scoring='r2',n_jobs=4,iid=False, cv=3)\n",
    "xgsearch5.fit(X_train,y_train)\n",
    "print(xgsearch5.best_params_)\n",
    "print(xgsearch5.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 8000, 'learning_rate': 0.005}\n",
      "0.9555914742619902\n"
     ]
    }
   ],
   "source": [
    "param_test6 = {\"learning_rate\":(.1,.05,.01,.005,.001,.0005,.0001),\"n_estimators\":(500,1000,2000,4000,8000)}\n",
    "xgsearch5 = GridSearchCV(estimator = XGBRegressor(max_depth=4,gamma=0.2,colsample_bytree=.75,subsample=.8,\n",
    "min_child_weight=3, nthread=4, scale_pos_weight=1,seed=27,reg_alpha=1e-05), \n",
    " param_grid = param_test6, scoring='r2',n_jobs=4,iid=False, cv=3)\n",
    "xgsearch5.fit(X_train,y_train)\n",
    "print(xgsearch5.best_params_)\n",
    "print(xgsearch5.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_xgb=XGBRegressor(n_estimators=8000,learning_rate=.005,max_depth=4,gamma=0.2,colsample_bytree=.75,subsample=.8,\n",
    "             min_child_weight=3, nthread=4, scale_pos_weight=1,seed=27,reg_alpha=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95554255011444855"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(estimator=final_xgb,X=np.array(X_train),y=y_train,scoring='r2',cv=3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.75,\n",
       "       gamma=0.2, learning_rate=0.005, max_delta_step=0, max_depth=4,\n",
       "       min_child_weight=3, missing=None, n_estimators=8000, nthread=4,\n",
       "       objective='reg:linear', reg_alpha=0.1, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=27, silent=True, subsample=0.8)"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_xgb.fit(np.array(X_train),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=3):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X.iloc[train_index], y.iloc[train_index])\n",
    "                y_pred = instance.predict(X.iloc[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred.ravel()\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_averaged_models = StackingAveragedModels(base_models = (final_gbm,final_xgb),meta_model = lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95529766808457006"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(estimator=stacked_averaged_models,X=pd.DataFrame(X_train),y=pd.DataFrame(y_train.ravel()),scoring='r2').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingAveragedModels(base_models=(GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.005, loss='ls', max_depth=7, max_features=2,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=3,\n",
       "             min_sam...near', reg_alpha=0.1, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=27, silent=True, subsample=0.8)),\n",
       "            meta_model=Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=0,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False),\n",
       "            n_folds=3)"
      ]
     },
     "execution_count": 666,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_averaged_models.fit(X=pd.DataFrame(X_train),y=pd.DataFrame(y_train.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_csv(\"Electricity_Production/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data=scaler_X.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=final_xgb.predict(np.array(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=.5*final_xgb.predict(np.array(test_data))+.3*final_gbm.predict(np.array(test_data))+.2*lin.predict(np.array(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions=scaler_y.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(predictions).to_csv(\"Electricity_Production/submission1.csv\",header=None,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1799.78984829],\n",
       "       [ 1818.33327689],\n",
       "       [ 1899.64542787],\n",
       "       [ 1909.8803624 ],\n",
       "       [ 1797.73698317],\n",
       "       [ 1777.45366792],\n",
       "       [ 1725.51570403],\n",
       "       [ 1805.64342495],\n",
       "       [ 1716.90919151],\n",
       "       [ 1782.71911878],\n",
       "       [ 1798.98506894],\n",
       "       [ 1862.37716931],\n",
       "       [ 1915.3103303 ],\n",
       "       [ 1910.66759775],\n",
       "       [ 1930.46133582],\n",
       "       [ 1818.65267473],\n",
       "       [ 1758.38153995],\n",
       "       [ 1773.13568202],\n",
       "       [ 1861.39321968],\n",
       "       [ 1776.37895701],\n",
       "       [ 1730.01220087],\n",
       "       [ 1775.98158298],\n",
       "       [ 1761.32776468],\n",
       "       [ 1784.07936602],\n",
       "       [ 1820.97528404],\n",
       "       [ 1791.89387756],\n",
       "       [ 1854.13351105],\n",
       "       [ 1804.23682591],\n",
       "       [ 1863.800172  ],\n",
       "       [ 1724.52046931],\n",
       "       [ 1907.84159941],\n",
       "       [ 1825.38732225],\n",
       "       [ 1767.76166782],\n",
       "       [ 1776.30977438],\n",
       "       [ 1771.50828271],\n",
       "       [ 1819.25226959],\n",
       "       [ 1818.52657777],\n",
       "       [ 1896.02160478],\n",
       "       [ 1868.41485459],\n",
       "       [ 1879.62912823],\n",
       "       [ 1772.51110381],\n",
       "       [ 1776.3930396 ],\n",
       "       [ 1893.09607266],\n",
       "       [ 1805.53356813],\n",
       "       [ 1788.05675961],\n",
       "       [ 1755.9190759 ],\n",
       "       [ 1727.71895611],\n",
       "       [ 1774.40841176],\n",
       "       [ 1771.67972567],\n",
       "       [ 1787.16994476],\n",
       "       [ 1923.94670751],\n",
       "       [ 1886.02548418],\n",
       "       [ 1906.74123732],\n",
       "       [ 1860.64319856],\n",
       "       [ 1777.52513028],\n",
       "       [ 1751.81381152],\n",
       "       [ 1936.48454228],\n",
       "       [ 1841.88263575],\n",
       "       [ 1877.71596333],\n",
       "       [ 1716.72703267],\n",
       "       [ 1861.53396672],\n",
       "       [ 1777.78861656],\n",
       "       [ 1795.11177717],\n",
       "       [ 1895.81885803],\n",
       "       [ 1853.93115701],\n",
       "       [ 1830.44864235],\n",
       "       [ 1879.36952785],\n",
       "       [ 1776.74746423],\n",
       "       [ 1753.20186331],\n",
       "       [ 1785.74006673],\n",
       "       [ 1866.132186  ],\n",
       "       [ 1731.94130135],\n",
       "       [ 1858.72548432],\n",
       "       [ 1818.53326324],\n",
       "       [ 1860.07482042],\n",
       "       [ 1747.94747289],\n",
       "       [ 1784.05533413],\n",
       "       [ 1810.74820018],\n",
       "       [ 1924.93761893],\n",
       "       [ 1860.53516029],\n",
       "       [ 1938.25170522],\n",
       "       [ 1754.08128568],\n",
       "       [ 1806.81908794],\n",
       "       [ 1931.40628513],\n",
       "       [ 1787.04804679],\n",
       "       [ 1882.53393568],\n",
       "       [ 1738.39227091],\n",
       "       [ 1850.6747338 ],\n",
       "       [ 1815.37480841],\n",
       "       [ 1865.30253819],\n",
       "       [ 1775.30891267],\n",
       "       [ 1768.5887273 ],\n",
       "       [ 1913.74061011],\n",
       "       [ 1785.42276385],\n",
       "       [ 1884.07370623],\n",
       "       [ 1862.99075038],\n",
       "       [ 1890.22614994],\n",
       "       [ 1811.90310523],\n",
       "       [ 1870.85704283],\n",
       "       [ 1874.55990547],\n",
       "       [ 1740.89099142],\n",
       "       [ 1809.18676777],\n",
       "       [ 1787.86577448],\n",
       "       [ 1819.5761283 ],\n",
       "       [ 1775.52082976],\n",
       "       [ 1766.61272245],\n",
       "       [ 1862.3279399 ],\n",
       "       [ 1774.34982508],\n",
       "       [ 1859.83792851],\n",
       "       [ 1902.98320361],\n",
       "       [ 1938.91152453],\n",
       "       [ 1766.31755492],\n",
       "       [ 1723.31687725],\n",
       "       [ 1736.47131918],\n",
       "       [ 1937.5864646 ],\n",
       "       [ 1752.41365749],\n",
       "       [ 1876.83413227],\n",
       "       [ 1822.80232204],\n",
       "       [ 1882.85858361],\n",
       "       [ 1730.80763211],\n",
       "       [ 1910.50957639],\n",
       "       [ 1906.44405004],\n",
       "       [ 1854.44283517],\n",
       "       [ 1798.89526727],\n",
       "       [ 1889.00108454],\n",
       "       [ 1886.34119324],\n",
       "       [ 1810.91836338],\n",
       "       [ 1851.42194645],\n",
       "       [ 1843.23251769],\n",
       "       [ 1722.58794425],\n",
       "       [ 1783.8511573 ],\n",
       "       [ 1739.86516813],\n",
       "       [ 1816.17683818],\n",
       "       [ 1854.43794511],\n",
       "       [ 1735.12972875],\n",
       "       [ 1767.86311401],\n",
       "       [ 1836.61747035],\n",
       "       [ 1769.711949  ],\n",
       "       [ 1871.13139152],\n",
       "       [ 1851.56895092],\n",
       "       [ 1888.39929144],\n",
       "       [ 1894.02853859],\n",
       "       [ 1872.77979306],\n",
       "       [ 1817.89709187],\n",
       "       [ 1762.81057128],\n",
       "       [ 1861.0962378 ],\n",
       "       [ 1720.88972949],\n",
       "       [ 1772.24498762],\n",
       "       [ 1919.77635049],\n",
       "       [ 1749.87698774],\n",
       "       [ 1731.66064787],\n",
       "       [ 1817.93405543],\n",
       "       [ 1836.03996791],\n",
       "       [ 1724.95636378],\n",
       "       [ 1814.79408159],\n",
       "       [ 1879.29447706],\n",
       "       [ 1843.20147388],\n",
       "       [ 1930.66001194],\n",
       "       [ 1757.47707634],\n",
       "       [ 1941.16108161],\n",
       "       [ 1801.78663329],\n",
       "       [ 1886.03962136],\n",
       "       [ 1793.12243651],\n",
       "       [ 1899.43168825],\n",
       "       [ 1758.43072024],\n",
       "       [ 1868.55136623],\n",
       "       [ 1863.61047252],\n",
       "       [ 1796.72423403],\n",
       "       [ 1937.20084846],\n",
       "       [ 1809.1836282 ],\n",
       "       [ 1803.86970926],\n",
       "       [ 1796.59096762],\n",
       "       [ 1744.77032121],\n",
       "       [ 1857.4718676 ],\n",
       "       [ 1734.46437406],\n",
       "       [ 1906.37201855],\n",
       "       [ 1774.64444858],\n",
       "       [ 1770.47907203],\n",
       "       [ 1735.91216651],\n",
       "       [ 1944.18103782],\n",
       "       [ 1950.0636426 ],\n",
       "       [ 1861.79689568],\n",
       "       [ 1777.56578101],\n",
       "       [ 1837.49145611],\n",
       "       [ 1809.68550621],\n",
       "       [ 1856.75746504],\n",
       "       [ 1735.14798092],\n",
       "       [ 1748.13653959],\n",
       "       [ 1868.20616048],\n",
       "       [ 1810.13477448],\n",
       "       [ 1770.12064983],\n",
       "       [ 1922.83181463],\n",
       "       [ 1892.15960366],\n",
       "       [ 1883.53566463],\n",
       "       [ 1907.83367697],\n",
       "       [ 1747.32199696],\n",
       "       [ 1753.11348712],\n",
       "       [ 1920.52962209],\n",
       "       [ 1737.3517616 ],\n",
       "       [ 1811.6819979 ]])"
      ]
     },
     "execution_count": 672,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.reshape(200,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
